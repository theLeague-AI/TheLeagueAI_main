{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Welcome to the Feature Engineering lesson. In this lesson we will be covering:\n",
    "\n",
    "- <b>Feature Engineering</b>\n",
    "    - Transformation of categorical and numerical variables\n",
    "    - Data imputation\n",
    "    - One hot encoding\n",
    "    - Binning\n",
    "- <b>Feature Selection</b>\n",
    "    - Applying domain knowledge\n",
    "    - Correlation\n",
    "- <b>Feature Scaling</b>\n",
    "    - Normalization\n",
    "    - Standardization\n",
    "\n",
    "The lab for Lesson 8 will consist of all the exercises that you will find throughout the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our goal will be to build a model that predicts whether a passenger survived the titanic.\n",
    "\n",
    "For this lesson we will again be using the Titanic Survival Dataset from Kaggle.\n",
    "\n",
    "Let's review the column values once more as a reminder of the data we are using:\n",
    "\n",
    "- Survived: Outcome of survival (0 = No; 1 = Yes)\n",
    "- Pclass: Socio-economic class (1 = Upper class; 2 = Middle class; 3 = Lower class)\n",
    "- Name: Name of passenger\n",
    "- Sex: Sex of the passenger\n",
    "- Age: Age of the passenger (Some entries contain ?)\n",
    "- SibSp: Number of siblings and spouses of the passenger aboard\n",
    "- Parch: Number of parents and children of the passenger aboard\n",
    "- Ticket: Ticket number of the passenger\n",
    "- Fare: Fare paid by the passenger\n",
    "- Cabin: Cabin number of the passenger (Some entries contain ?)\n",
    "- Embarked: Port of embarkation of the passenger (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "- Boat: Lifeboat (if survived)\n",
    "- Body: Body Number (if did not survive and body was recovered)\n",
    "- Home.Dest: Home / Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import titanic dataset\n",
    "titanic_data = pd.read_csv(\"titanic_data.csv\")\n",
    "titanic_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we start our feature engineering process, we will need to handle our missing values.\n",
    "We will need to first get rid of the question marks. Then we will take a look at the percentage of null values.\n",
    "\n",
    "#### Review from lesson 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace ? with none\n",
    "titanic_data = titanic_data.replace({'?': None})\n",
    "\n",
    "# change the type of the age and fare columns to numeric\n",
    "titanic_data['age'] = pd.to_numeric(titanic_data['age'], errors = 'coerce')\n",
    "titanic_data['fare'] = pd.to_numeric(titanic_data['fare'], errors = 'coerce')\n",
    "\n",
    "# Lets round the age values\n",
    "titanic_data['age'] = round(titanic_data['age'])\n",
    "\n",
    "# lets check for null values again\n",
    "titanic_data.isnull().sum()/len(titanic_data)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are a few features that a large amount of nulls. \n",
    "\n",
    "***Should we get rid of them?*** ---> This is one example of how we are conducting <b>feature engineering</b> by applying our domain knowledge.\n",
    "\n",
    "In this case, there is no right or wrong percentage threshold. However, if a large portion of data is missing, it may be best to remove the column since imputating may add bias to our models. But before we do that...\n",
    "\n",
    "We need to first ask ourselves the reason behind why the data may be missing. \n",
    "\n",
    "- Cabin: we may assume that the missing values are because some passengers may not have had cabins, which could tell you something about whether or not they survived (missing data = perhaps no cabin, survived)\n",
    "- Embarked: we may assume that missing values are because some passengers may not have given this info (small percentage)\n",
    "- Boat: indicates whether or not they survived (missing data = perhaps did not survive)\n",
    "- Body: indicates whether or not they survived (missing data = perhaps survived)\n",
    "- Home.Dest: we may assume that missing values are because some passengers may not have given this info \n",
    "\n",
    "Since the only variable with large missing value percentage not associated with survival is home.dest, let's look at the number of occurrences to see if we see any pattern. If there is a pattern, we can feature engineer this column to be more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the percentage of occurrences \n",
    "\n",
    "titanic_data[\"home.dest\"].value_counts()/titanic_data[\"home.dest\"].value_counts().sum()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The home/destination with the highest percentage is New York, NY with 8.6% of passengers arriving at this destination. The total number of locations is 369. \n",
    "- Since this is a large spread and the highest occurrence is less than 10%, it is safe to assume there are no patterns detected and we can remove this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the home.dest column\n",
    "titanic_data = titanic_data.drop(columns=['home.dest'])\n",
    "\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "We dropped the \"home.dest\" column, but if we wanted to keep it, what could we have done to make this column useful to predict whether a passenger survived? \n",
    "\n",
    "Hint: What kind of information does the home or destination give you about a passenger? \n",
    "\n",
    "Hint Hint: Google the following cities: \n",
    "  - Southhampton, UK\n",
    "  - Queenstone, UK\n",
    "  - Cherbourg, FR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Double click here)\n",
    "\n",
    "Response Exercise 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformation\n",
    "\n",
    "Now let's consider the other 3 out of 4 variables - cabin, boat, and body. Since these features may help us predict survival, we can transform them so they are more useful to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of using cabin number, let's create a new column showing whether or not the passenger had a cabin\n",
    "\n",
    "titanic_data['has_cabin'] = ~titanic_data.cabin.isnull()\n",
    "\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now repeat this process using the boat and body columns \n",
    "\n",
    "titanic_data['has_boat'] = ~titanic_data.boat.isnull()\n",
    "titanic_data['has_body'] = ~titanic_data.body.isnull()\n",
    "\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the name column, we see there are titles within the names such as Miss, Master, Mr., Mrs., etc. \n",
    "\n",
    "We can conduct feature engineering on this column by creating a new column showing the title. This information may be useful to understand social status, profession, etc. which could help us understand the passengers' chances of survival.\n",
    "\n",
    "We can use regular expressions to extract the title from each name. You can visit these links to learn more about regular expressions: \n",
    "- https://docs.python.org/3/howto/regex.html \n",
    "- https://docs.python.org/3/library/re.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting title from the names and storing into a new column \n",
    "import re\n",
    "\n",
    "titanic_data['title'] = titanic_data.name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the title column to see how this looks\n",
    "titanic_data['title'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are several titles that are less common. To help our model with its prediction, we can group the less common titles into one single group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping all columns as \"Other\"\n",
    "titanic_data['title'] = titanic_data['title'].replace(['Don', 'Dona', 'Rev', 'Dr',\n",
    "                    'Major', 'Lady', 'Sir', 'Col', 'Capt', 'Countess', 'Jonkheer', 'Ms', 'Mlle', 'Mme'],'Other')\n",
    "\n",
    "#Plotting the results\n",
    "titanic_data['title'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now drop the irrelevant columns that are no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the name, cabin, boat, body, ticket columns\n",
    "titanic_data = titanic_data.drop(columns=['name','ticket', 'cabin', 'boat', 'body'])\n",
    "\n",
    "titanic_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the embarked column. Since there is only less than 1% of missing data, we can impute the missing values using the most common value - i.e. the mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many missing values do we have?\n",
    "titanic_data.embarked.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the other values? \n",
    "titanic_data.embarked.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the most common occurrence? \n",
    "titanic_data.embarked.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can impute the missing values by replacing None with the mode\n",
    "titanic_data['embarked'].fillna(titanic_data.embarked.mode()[0], inplace=True)\n",
    "\n",
    "titanic_data.embarked.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Feature engineer a new column by combining the \"sibsp\" and \"parch\" columns. You can call it \"Family Size\".\n",
    "\n",
    "Hint: Recall back on your pandas lessons (aka use pandas to combine these columns). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write Code Here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "\n",
    "#### Transforming categorical variables into numerical variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look at the dataframe\n",
    "titanic_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new dataframe that contains dummy variables \n",
    "titanic_data = pd.get_dummies(titanic_data, drop_first=True)\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congratulations, we've now completed a few feature engineering techniques! \n",
    "\n",
    "#### But wait........ there's more!\n",
    "\n",
    "Now let's go back to our dataframe and see what other features still contain missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check for null values again\n",
    "titanic_data.isnull().sum()/len(titanic_data)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the 2 variables \n",
    "\n",
    "fg = sns.FacetGrid(titanic_data, col='survived')\n",
    "fg.map(sns.histplot, \"age\", bins=20)\n",
    "fg.add_legend()\n",
    "\n",
    "\n",
    "fg = sns.FacetGrid(titanic_data, col='survived')\n",
    "fg.map(sns.histplot, \"fare\", bins=20)\n",
    "fg.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's impute missing data for the remaining 2 numerical features.\n",
    "\n",
    "Let's replace \"age\" and \"fare\" columns with the median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values for 'age' and 'fare'\n",
    "titanic_data['age'] = titanic_data.age.fillna(titanic_data.age.mean())\n",
    "titanic_data['fare'] = titanic_data.fare.fillna(titanic_data.fare.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifying that all missing values were taken care of\n",
    "titanic_data.isnull().sum()/len(titanic_data)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the remaining 2 numerical variables - age and fare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using facetgrid to plot based on survival\n",
    "fg = sns.FacetGrid(titanic_data, col='survived')\n",
    "fg.map(sns.histplot, \"age\", bins=20)\n",
    "fg.add_legend()\n",
    "\n",
    "fg = sns.FacetGrid(titanic_data, col='survived')\n",
    "fg.map(sns.histplot, \"fare\", bins=20)\n",
    "fg.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning Numerical Data\n",
    "\n",
    "One technique used when dealing with ranges of numerical data is creating bins that reflect patterns in the data. This is one way to include outliers which in turn may create noise. Binning allows you to put observations within a certain range in the same bin. \n",
    "\n",
    "In this case, we can use the pandas function **`qcut()`** to bin the age column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binning the age column \n",
    "titanic_data['age_bin'] = pd.qcut(titanic_data.age, q=4, labels=False )\n",
    "\n",
    "#Plotting this column\n",
    "titanic_data['age_bin'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Exercise 3 \n",
    "\n",
    "#Using the qcut method, create a new bin column using \"fare\" that contains 10 bins\n",
    "\n",
    "\n",
    "\n",
    "#Plot new column\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "#### Normalization\n",
    "\n",
    "- Normalization rescales the values so they are between the [0,1] range. \n",
    "- It subtracts the minimum value from each value and divides it by the maximum minus the minimum value. \n",
    "- It is also known as Min-Max scaling.\n",
    "\n",
    "<img src=\"Normalization Formula.PNG\">\n",
    "\n",
    "We can use the <b>min()</b> and <b>max()</b> methods in pandas to normalize our age column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying normalization technique by using pandas min and max methods\n",
    "\n",
    "# Find the max and min from the age column\n",
    "max_age = titanic_data.age.max()\n",
    "min_age = titanic_data.age.min()\n",
    "\n",
    "# Use the Mapping Function\n",
    "titanic_data['age'] = titanic_data.age.map(lambda p: (p - min_age)/(max_age - min_age))\n",
    "  \n",
    "# View normalized data\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the age column to see our new range of values\n",
    "import plotly.express as px\n",
    "\n",
    "hist = px.histogram(titanic_data,x = \"age\", opacity = 0.7)\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization\n",
    "\n",
    "- Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. \n",
    "- This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation.\n",
    "- Standardized Measurement = (original measurement – mean of the variable) divided by (standard deviation of the variable)\n",
    "\n",
    "<img src=\"Standardization Formula.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying standardization technique by using pandas mean and standard deviation methods\n",
    "column = 'fare'\n",
    "data[column] = (data[column] - data[column].mean()) / data[column].std()    \n",
    "  \n",
    "# View normalized data  \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the fare column to see our new range of values\n",
    "fare_hist = px.histogram(data,x = \"fare\", opacity = 0.7)\n",
    "fare_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation \n",
    "\n",
    "- Spearman and Pearson are two statistical methods used to calculate the strength of correlation between 2 variables. \n",
    "\n",
    "- <b>Pearson Correlation Coefficient </b> can be used with continuous variables that have linear relationship. \n",
    "\n",
    "- <b>Spearman Correlation Coefficient </b> can be used when you have a non-linear relationship or ordinal categorical variables.\n",
    "\n",
    "In this case, we will be using correlation techniques to select the best feature for our model --> <b>feature selection</b>!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heatmap(data):\n",
    "    correlations = data.corr()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,20))\n",
    "    sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f',\n",
    "                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .70})\n",
    "    plt.show();\n",
    "    \n",
    "correlation_heatmap(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid multicollinearity, we must now remove features with high correlation:\n",
    "- age OR age_bin\n",
    "- has_boat OR survived\n",
    "- title_Mr OR sex_male"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations Future Data Scientist/Machine Learning Engineer! \n",
    "\n",
    "## You've now added many awesome techniques to your toolbox. \n",
    "\n",
    "## You are one step closer to the $1,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
