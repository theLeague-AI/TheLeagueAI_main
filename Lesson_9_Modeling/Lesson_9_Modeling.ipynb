{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9 - Modeling\n",
    "Welcome to the Modeling. In this lesson we will be covering: \n",
    "- **What is modeling**\n",
    "- **How does modeling differ from algorithms**\n",
    "- **Types of Models** \n",
    "- **Sklearn Library**\n",
    "- **Building towards a model**\n",
    "- **Modeling**\n",
    "\n",
    "The lab for Lesson 9 will consist of all the exercises throughtout the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lesson we will again be using the Titanic Survival Dataset from Kaggle to predict survival of passengers.\n",
    "\n",
    "Let's review the column values once more as a reminder of the data we are using:\n",
    "\n",
    "- **Survived**: Outcome of survival (0 = No; 1 = Yes)\n",
    "- **Pclass**: Socio-economic class (1 = Upper class; 2 = Middle class; 3 = Lower class)\n",
    "- **Name**: Name of passenger\n",
    "- **Sex**: Sex of the passenger\n",
    "- **Age**: Age of the passenger (Some entries contain ?)\n",
    "- **SibSp**: Number of siblings and spouses of the passenger aboard\n",
    "- **Parch**: Number of parents and children of the passenger aboard\n",
    "- **Ticket**: Ticket number of the passenger\n",
    "- **Fare**: Fare paid by the passenger\n",
    "- **Cabin**: Cabin number of the passenger (Some entries contain ?)\n",
    "- **Embarked**: Port of embarkation of the passenger (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "- **Boat**: Lifeboat (if survived)\n",
    "- **Body**: Body Number (if did not survive and body was recovered)\n",
    "- **Home.Dest**: Home / Destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you do not have the Scikit-learn, please install using the following command. \n",
    "# If you do not need to, then you can skip this step\n",
    "!conda install -c anaconda scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your model ML - End to End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our lab today, we will go through end to end of the machine learning process to reiterate concepts we have learned, and then discuss ones that we learned today. \n",
    "\n",
    "Lets start with importing our libraries...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "# Import the pandas library \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 read in the titanic dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now explore our data to get a feel of what we are dealing with. Remember this is typically the first step of the process, if you do not understand your data, then you will not be able to predict with your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 - Show the column types of the Titanic Data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we must group the dataset by Pclass and Survived to gather the total count\n",
    "group = titanic_data.groupby(['pclass', 'survived'])\n",
    "pclass_survived = group.size().unstack()\n",
    "\n",
    "\n",
    "# Creating a histogram of age by survival\n",
    "hist = px.histogram(titanic_data,x = \"age\", opacity = 0.7, color = \"survived\")\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4** - How else could we explore the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Double Click Here) Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus - Try exploring another feature from the titanic dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I will demonstrate how I explored the data.The first thing you will notice is that I created two list:\n",
    "\n",
    "- categorical columns \n",
    "- numerical columns\n",
    "\n",
    "I do this, to make it easier for me to access columns from these groups and if I need to apply any transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"name\",\"sex\",\"cabin\",\"embarked\", \"home.dest\"]\n",
    "numerical_columns = [\"pclass\",\"survived\",\"age\",\"sibsp\",\"parch\",\"ticket\",\"fare\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will look at the values from my categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in categorical_columns:\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"For column {}, the values are: \\n{} \\n\".format(column, titanic_data[column].value_counts()))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4** - What can you understand from the above `value_counts()`? Do you see alot of people from NY or does everyone have the same name? Describe what you see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Double Click Here) Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at how our numerical values are distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in numerical_columns:\n",
    "    titanic_data_final.hist(column=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5** - Do you believe any of the numerical values will have an effect on our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Double Click Here) Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to start preparing our data. Here we will transform columns or clean up the data before we begin to feature engineer any new columns. \n",
    "\n",
    "Lets start with replacing our good ole friend the \"?\", so that we can decide how to replace missing values later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data = titanic_data.replace({'?': None})\n",
    "titanic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets count how many missing values we currently have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the step below we can see where I use the list of numerical columns to convert them all to numeric. You could have typed each individual column out, but this way you save time and can focus at the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numeric columns to numeric\n",
    "titanic_data[numerical_columns] = titanic_data[numerical_columns].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step I shorten the cabin values to only the cabin letter and not the number. I am doing this to make our next steps easier, but we are losing information that could be important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data[\"cabin\"] = titanic_data[\"cabin\"].str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in our previous lesson, Feature engineering is used to enhance our data to draw more insights. Here we will transform stings to numbers, potentially remove missing data, one hot encode data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding cabin\n",
    "deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"T\": 8}\n",
    "titanic_data[\"cabin\"] = titanic_data[\"cabin\"].map(deck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding sex\n",
    "titanic_data = pd.get_dummies(titanic_data,columns= [\"sex\"], prefix=\"gender\")\n",
    "titanic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below I am implementing a feature engineering technique called **target mean encoding**. We will only glance at the code, but I wanted to give you a demonstration of more complex techniques that are used in industry "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = titanic_data[\"survived\"].mean()\n",
    "embarked_agg = titanic_data.groupby(\"embarked\")[\"survived\"].agg(['count','mean'])\n",
    "counts = embarked_agg['count']\n",
    "means = embarked_agg['mean']\n",
    "target_mean = (counts * means + 100 * means)/(counts + 100)\n",
    "titanic_data[\"embarked\"] = titanic_data[\"embarked\"].map(target_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets deal with our missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above we have alot of missing values. Our dataset has about 1309 values, and we have cabin and body with more than 1000 values missing. It is here where we can validate if dropping values or imputing will make a difference in our final results. In the cell below, add or remove columns you believe will affect our model due to missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will drop columns we deem unnecessary. Remember this spot as we will come back \n",
    "# Exercise 6 -  specify columns to drop\n",
    "\n",
    "titanic_data = titanic_data.drop(columns=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have removed columns that we have deemed unnecessary, lets fill in values using one of the following:\n",
    "\n",
    "- **mean**: replace missing values using the mean along each column. Can only be used with numeric data.\n",
    "\n",
    "- **median**: replace missing values using the median along each column. Can only be used with numeric data.\n",
    "\n",
    "- **most_frequent**: replace missing using the most frequent value along each column. Can be used with strings or numeric data. If there is more than one such value, only the smallest is returned.\n",
    "\n",
    "- **constant**:  replace missing values with fill_value. Can be used with strings or numeric data.\n",
    "\n",
    "We will be using the `SimpleImputer` method from the sklearn library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7 - specify a method for filling in missing data \n",
    "from sklearn.impute import SimpleImputer\n",
    "# specify method below after strategy=. Mean will be the default value \n",
    "imp = SimpleImputer(strategy=\"mean\")\n",
    "imp.fit(titanic_data)\n",
    "titanic_data_final = imp.transform(titanic_data)\n",
    "titanic_data_final = pd.DataFrame(titanic_data_final, columns=titanic_data.columns)\n",
    "titanic_data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rounding the cabin value if we did not drop it\n",
    "if \"cabin\" in titanic_data.columns:\n",
    "    titanic_data_final['cabin'] = round(titanic_data_final['cabin'])\n",
    "    titanic_data_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_final.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went over all of the above to emphasize that without good data, we do not have a good model. All of the above is part of the modeling process. \n",
    "\n",
    "Now that we have our **final titanic  dataset** ready we can now approach the steps to model our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Modeling\n",
    "As mentioned from the lecture we have:\n",
    "\n",
    "- Unsupervised Models\n",
    "    - Clustering\n",
    "    - Dimensionality Reduction\n",
    "- Supervised Models\n",
    "    - Regression\n",
    "    - Classification\n",
    "\n",
    "We are focusing on the supervised models.\n",
    "\n",
    "**Classification**: Models are used to classify your target variable. Such as, predict what color my shirt is, or predict if a transaction is fradualent or not. Classification is used to determine a label\n",
    "\n",
    "**Regression**: Models are used to predict values. Such as Predict the stock price for Apple tomorrow, or predict my yearly income for next year. \n",
    "\n",
    "Each both have various specific models under each of them. A few of them are Neural Networks, LSTMS, RandomForrest, XGBoost, DecesionTrees, PCA, K-Means and various others. All these model have the advantages, disadvantages, and specific uses, but we will go over these in another lesson. If you would like to get a head start, you can take a look at the [sklearn documentation](https://scikit-learn.org/stable/user_guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to remove our value that we aim to predict. We do this because then the model will have no way to know what we are asking of it. If we did not specify our target variable, it would be similiar to expecting ice cream, without asking for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = titanic_data_final[\"survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After specifying the target variable, we now drop it from the titanic_daata_final, so then we can have a dataset of just our features. What are features? \n",
    "\n",
    "**Features:** Measurable values to be analyzed. \n",
    "\n",
    "An easier way to understand features is to imagine them as the inputs(columns) to an algorithm that helps make the decisions or path to take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = titanic_data_final.drop('survived', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check what could be good features\n",
    "Now that we have our features we could use modeling to help us decide which features to use in our final model. We will not go over the code below in detail, but we will use the graph to see what might be good features to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "feature_weights = model.feature_importances_\n",
    "feature_weights\n",
    "\n",
    "# Show top 5 features \n",
    "indices = np.argsort(feature_weights)[::-1]\n",
    "columns = X_train.columns.values[indices[:5]]\n",
    "values = feature_weights[indices][:5]\n",
    "\n",
    "# Creat the plot\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "plt.title(\"Normalized Weights for First Five Most Predictive Features\", fontsize=16)\n",
    "plt.bar(np.arange(5), values, width=0.6, align=\"center\", color='#00A000', \\\n",
    "       label=\"Feature Weight\")\n",
    "plt.bar(np.arange(5) - 0.3, np.cumsum(values), width=0.2, align=\"center\", color='#00A0A0', \\\n",
    "       label=\"Cumulative Feature Weight\")\n",
    "plt.xticks(np.arange(5), columns)\n",
    "plt.xlim((-0.5, 4.5))\n",
    "plt.ylabel(\"Weight\", fontsize=12)\n",
    "plt.xlabel(\"Feature\", fontsize=12)\n",
    "\n",
    "plt.legend(loc='upper center')\n",
    "plt.tight_layout()\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:** What features appeared for you? Why do you believe they were your top features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Double Click Here) Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Train and Test\n",
    "Before we submit out featres and target to a model, we first need to split our data into a test set and a training set. We split our data into test/train, because if we were to train our model with the complete dataset, how could we test our model? How could we know if our model is better than just guessing. \n",
    "\n",
    "By splitting the data we are providing a system of checks and balances, so that if we were to use this model in real life, we could verify our model to more than what is included in our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `train_test_split` method from the sklearn library to split our data 70% training and 30% testing. You can play with this to see how your results vary. Just change the **test_size** input to a decimal value from .1-.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test. \n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets take a quick look at our train features values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets take a quick look at our train target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Model\n",
    "\n",
    "We are finally at the step to creating our model. As this will be your second time viewing a model, we will mainly use the default settings. Models typically have a variety of levers that can be used, but we will cover this in another lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the classifier from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "# Define the classifier, and fit it to the data. Here we are creating a variable type of the DecisionTreeClassifier\n",
    "# that we will use to perform actions with. \n",
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to see all of the actions that you can take with [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) follow the link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "We will now train our model. Taining our model will use our training set created from `train_test_split`. This step can be viewed as teaching our model. The model than stores memory as weights, which will then be used for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a graph of how are model made decisions. As you can see it made lots of decisions based off of the data we provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict using your model\n",
    "Now we will ask our model to guess values from our testing set. The model has not seen these values, so it is a good test to see how we are doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check our results\n",
    "Below we are the results for our model. Our next lesson is model accuracy, so we will dive deeper then. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score\n",
    "\n",
    "train_accuracy = balanced_accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = balanced_accuracy_score(y_test, y_test_pred)\n",
    "train_precision = precision_score(y_train, y_train_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "train_recall = recall_score(y_train, y_train_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "\n",
    "print('The training accuracy is {}%'.format(train_accuracy*100))\n",
    "print('The training precision is {}%'.format(train_precision*100))\n",
    "print('The training recall is {}%'.format(train_recall*100))\n",
    "print('The test accuracy is {}%'.format(test_accuracy * 100))\n",
    "print('The test precision is {}%'.format(test_precision * 100))\n",
    "print('The test recall is {}%'.format(test_recall * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for now how do you think we did? Could we model our data better with better features? Lets take another look with the code below. \n",
    "\n",
    "#### Another way to Model\n",
    "\n",
    "We have created the code below to show you, one of the levers DecisionTreeClassifier has at its disposal. We are looping over values from 1-20 of max_depth, to show you how adjusting one of these levers can change your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "tree_values = [i for i in range(1, 21)]\n",
    "\n",
    "for i in tree_values:\n",
    "    # configure the model, this is where we set our lever\n",
    "    model = DecisionTreeClassifier(max_depth=i)\n",
    "    # fit model on the training dataset\n",
    "    model.fit(X_train, y_train)\n",
    "    # evaluate on the train dataset\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_acc = balanced_accuracy_score(y_train, y_train_pred)\n",
    "    train_scores.append(train_acc)\n",
    "    # evaluate on the test dataset\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_acc = balanced_accuracy_score(y_test, y_test_pred)\n",
    "    test_scores.append(test_acc)\n",
    "    # summarize progress\n",
    "    print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how our model performed in training and testing when we moved through the different values for max_depth. As you can see our model could still use some improvement, but we are performing better than we did from lesson 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tree_values, train_scores, '-o', label='Train')\n",
    "plt.plot(tree_values, test_scores, '-o', label='Test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9**: What do you believe could make our model better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Double Click Here) Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus**: Try to improve our model to 90% and describe what you did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Double Click Here) Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations Future Data Scientist/Machine Learning Engineer! \n",
    "\n",
    "## You've now added many awesome techniques to your toolbox. \n",
    "\n",
    "### You are so close to winning the grand prize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
